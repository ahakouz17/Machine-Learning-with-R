class= y_truth[s]
value= value+ y_truth[i] * log(y_predicted[i,class] + 1e-100)
}
}
objective_values <- c(objective_values,- value)
if ( abs(objective_values[iteration + 1] - objective_values[iteration]) < epsilon || iteration >= max_iteration) {
break
}
iteration <- iteration + 1
}
plot(1:(iteration + 1),objective_values,
type = "l", lwd = 2, las = 1,
xlab = "Iteration", ylab = "Error")
### Machine Learning HW3 Developed by: Daliyah Aljamal,
##ID: 0063583
library(MASS)
set.seed(421)
# mean parameters
class_means <- array(c(2.0, 2.0,
-4.0,-4.0,
-2.0,2.0,
4.0, -4.0,
-2.0,-2.0,
4.0,4.0,
2.0,-2.0,
-4.0,4.0),c(2,1,8))
# Variance parameters
class_covariances <- array(c(+0.8, -0.6, -0.6,+0.8,
+0.4,+0.0,+0.0,+0.4,
+0.8,+ 0.6, +0.6,+0.8,
+0.4, +0.0,+0.0,+0.4,
+0.8, -0.6, -0.6, +0.8,
+0.4,+0.0,+0.0,+0.4,
+0.8,+0.6,+0.6,+0.8,
+0.4,+0.0,+0.0,+0.4), c(2,2,8))
# sample sizes
class_sizes <- c(100,100,100,100)
#Generate arandom Data
points1 <- mvrnorm(n = class_sizes[1]/2, mu = class_means[,,1], Sigma = class_covariances[,,1])
points2 <- mvrnorm(n = class_sizes[1]/2, mu = class_means[,,2], Sigma = class_covariances[,,2])
points3 <- mvrnorm(n = class_sizes[2]/2, mu = class_means[,,3], Sigma = class_covariances[,,3])
points4 <- mvrnorm(n = class_sizes[2]/2, mu = class_means[,,4], Sigma = class_covariances[,,4])
points5 <- mvrnorm(n = class_sizes[3]/2, mu = class_means[,,5], Sigma = class_covariances[,,5])
points6 <- mvrnorm(n = class_sizes[3]/2, mu = class_means[,,6], Sigma = class_covariances[,,6])
points7 <- mvrnorm(n = class_sizes[4]/2, mu = class_means[,,7], Sigma = class_covariances[,,7])
points8 <- mvrnorm(n = class_sizes[4]/2, mu = class_means[,,8], Sigma = class_covariances[,,8])
X <- rbind(points1, points2, points3, points4,points5,points6,points7,points8)
colnames(X) <- c("x1", "x2")
# generate corresponding labels
y <- c(rep(1, class_sizes[1]), rep(2, class_sizes[2]), rep(3, class_sizes[3]), rep(4, class_sizes[4]))
# write data to a file
write.csv(x = cbind(X, y), file = "ML_HW3.csv", row.names = FALSE)
# plot data points generated
plot(points1[,1], points1[,2], type = "p", pch = 19, col = "red", las = 1,
xlim = c(-6, 6), ylim = c(-6, 6),
xlab = "x1", ylab = "x2")
points(points2[,1], points2[,2], type = "p", pch = 19, col = "red")
points(points3[,1], points3[,2], type = "p", pch = 19, col = "green")
points(points4[,1], points4[,2], type = "p", pch = 19, col = "green")
points(points5[,1], points5[,2], type = "p", pch = 19, col = "blue")
points(points6[,1], points6[,2], type = "p", pch = 19, col = "blue")
points(points7[,1], points7[,2], type = "p", pch = 19, col = "pink")
points(points8[,1], points8[,2], type = "p", pch = 19, col = "pink")
# read data into memory
data_set <- read.csv("ML_HW3.csv")
# get X and y values
X <- cbind(data_set$x1, data_set$x2)
y_truth <- data_set$y
# get number of samples and number of features
N <- length(y_truth)
D <- ncol(X)
num_classes=max(y_truth)
#Define sigmoid Function
# define the sigmoid function
sigmoid <- function(a) {
return (1 / (1 + exp(-a)))
}
#Define Softmax Function
# #softmax output function
softmax<-function(X){
eps=1e-15
Eps=1-eps
M=max(X)
product=apply(X,2,function(x) exp(-M-log(rowSums(exp(X-M-x)))))
product=pmax(product,eps)
product=pmin(product,Eps)
return(product)
}
#MY SOFTMAX
my_softmax <- function(X,w, c)
{
sum <- 0
for (i in 1:num_classes)
{
sum <- sum+ exp(X %*% w[,i])
#next
}
return (exp(X %*% w[,c])/sum )
}
# set learning parameters
eta <- 0.1
epsilon <- 1e-3
H <- 20
max_iteration <- 200
# randomly initalize W and v
set.seed(421)
W <- matrix(runif((D + 1) * H, min = -0.01, max = 0.01), D + 1, H)
v <- matrix(runif((H + 1) * num_classes, min = -0.01, max = 0.01), H + 1, num_classes)
#Hidden Nodes
Z <- sigmoid(cbind(1,X) %*% W)
#Output Nodes
#y_predicted= my_softmax(cbind(1,Z),v,1)
y_predicted <- sapply(1:num_classes, function(i){my_softmax(cbind(rep(1, N), Z), v, i)})
objective_value =0
for (i in 1:N){
objective_value <- objective_value + log(y_predicted[i, y_truth[i]] + 1e-100)
}
objective_values= c(-objective_value)
# learn W and v using gradient descent and online learning
iteration <- 1
while (1) {
print(paste0("running iteration#", iteration))
value=0
for (i in sample(N)) {
# calculate hidden nodes
Z[i,] <- sigmoid(c(1, X[i,]) %*% W)
# calculate output nodes
for (o in 1:num_classes){
y_predicted[i,o]= my_softmax(matrix(c(1, Z[i,]),1, H+1), v, o)
}
for (o in 1:num_classes){
if(y_truth[i] != o){
v[,o] <- v[,o] + eta * (0 - y_predicted[i, o]) * c(1, Z[i,])
} else {
v[,o] <- v[,o] + eta * (1 - y_predicted[i,o])* c(1, Z[i,])
}
}
for (h in 1:H){
#Intilizing cal value to do first summation
cal=0
for (t in 1:num_classes){
if(y_truth[i] == t){
cal= cal + (1 - y_predicted[i, t]) * v[h,t]
} else {
cal <- cal - y_predicted[i, t] * v[h,t]
}
}
W[,h] <- W[,h] + eta * cal * Z[i, h] * (1 - Z[i, h]) * c(1, X[i,])
}
#Calculating error function
for (s in 1:N){
class= y_truth[s]
value= value+ log(y_predicted[i,class] + 1e-100)
}
}
objective_values <- c(objective_values,- value)
if ( abs(objective_values[iteration + 1] - objective_values[iteration]) < epsilon || iteration >= max_iteration) {
break
}
iteration <- iteration + 1
}
plot(1:(iteration + 1),objective_values,
type = "l", lwd = 2, las = 1,
xlab = "Iteration", ylab = "Error")
### Machine Learning HW3 Developed by: Daliyah Aljamal,
##ID: 0063583
library(MASS)
set.seed(421)
# mean parameters
class_means <- array(c(2.0, 2.0,
-4.0,-4.0,
-2.0,2.0,
4.0, -4.0,
-2.0,-2.0,
4.0,4.0,
2.0,-2.0,
-4.0,4.0),c(2,1,8))
# Variance parameters
class_covariances <- array(c(+0.8, -0.6, -0.6,+0.8,
+0.4,+0.0,+0.0,+0.4,
+0.8,+ 0.6, +0.6,+0.8,
+0.4, +0.0,+0.0,+0.4,
+0.8, -0.6, -0.6, +0.8,
+0.4,+0.0,+0.0,+0.4,
+0.8,+0.6,+0.6,+0.8,
+0.4,+0.0,+0.0,+0.4), c(2,2,8))
# sample sizes
class_sizes <- c(100,100,100,100)
#Generate arandom Data
points1 <- mvrnorm(n = class_sizes[1]/2, mu = class_means[,,1], Sigma = class_covariances[,,1])
points2 <- mvrnorm(n = class_sizes[1]/2, mu = class_means[,,2], Sigma = class_covariances[,,2])
points3 <- mvrnorm(n = class_sizes[2]/2, mu = class_means[,,3], Sigma = class_covariances[,,3])
points4 <- mvrnorm(n = class_sizes[2]/2, mu = class_means[,,4], Sigma = class_covariances[,,4])
points5 <- mvrnorm(n = class_sizes[3]/2, mu = class_means[,,5], Sigma = class_covariances[,,5])
points6 <- mvrnorm(n = class_sizes[3]/2, mu = class_means[,,6], Sigma = class_covariances[,,6])
points7 <- mvrnorm(n = class_sizes[4]/2, mu = class_means[,,7], Sigma = class_covariances[,,7])
points8 <- mvrnorm(n = class_sizes[4]/2, mu = class_means[,,8], Sigma = class_covariances[,,8])
X <- rbind(points1, points2, points3, points4,points5,points6,points7,points8)
colnames(X) <- c("x1", "x2")
# generate corresponding labels
y <- c(rep(1, class_sizes[1]), rep(2, class_sizes[2]), rep(3, class_sizes[3]), rep(4, class_sizes[4]))
# write data to a file
write.csv(x = cbind(X, y), file = "ML_HW3.csv", row.names = FALSE)
# plot data points generated
plot(points1[,1], points1[,2], type = "p", pch = 19, col = "red", las = 1,
xlim = c(-6, 6), ylim = c(-6, 6),
xlab = "x1", ylab = "x2")
points(points2[,1], points2[,2], type = "p", pch = 19, col = "red")
points(points3[,1], points3[,2], type = "p", pch = 19, col = "green")
points(points4[,1], points4[,2], type = "p", pch = 19, col = "green")
points(points5[,1], points5[,2], type = "p", pch = 19, col = "blue")
points(points6[,1], points6[,2], type = "p", pch = 19, col = "blue")
points(points7[,1], points7[,2], type = "p", pch = 19, col = "pink")
points(points8[,1], points8[,2], type = "p", pch = 19, col = "pink")
# read data into memory
data_set <- read.csv("ML_HW3.csv")
# get X and y values
X <- cbind(data_set$x1, data_set$x2)
y_truth <- data_set$y
# get number of samples and number of features
N <- length(y_truth)
D <- ncol(X)
num_classes=max(y_truth)
#Define sigmoid Function
# define the sigmoid function
sigmoid <- function(a) {
return (1 / (1 + exp(-a)))
}
#Define Softmax Function
# #softmax output function
softmax<-function(X){
eps=1e-15
Eps=1-eps
M=max(X)
product=apply(X,2,function(x) exp(-M-log(rowSums(exp(X-M-x)))))
product=pmax(product,eps)
product=pmin(product,Eps)
return(product)
}
#MY SOFTMAX
my_softmax <- function(X,w, c)
{
sum <- 0
for (i in 1:num_classes)
{
sum <- sum+ exp(X %*% w[,i])
#next
}
return (exp(X %*% w[,c])/sum )
}
# set learning parameters
eta <- 0.1
epsilon <- 1e-3
H <- 20
max_iteration <- 200
# randomly initalize W and v
set.seed(421)
W <- matrix(runif((D + 1) * H, min = -0.01, max = 0.01), D + 1, H)
v <- matrix(runif((H + 1) * num_classes, min = -0.01, max = 0.01), H + 1, num_classes)
#Hidden Nodes
Z <- sigmoid(cbind(1,X) %*% W)
#Output Nodes
#y_predicted= my_softmax(cbind(1,Z),v,1)
y_predicted <- sapply(1:num_classes, function(i){my_softmax(cbind(rep(1, N), Z), v, i)})
objective_value =0
for (i in 1:N){
objective_value <- objective_value + log(y_predicted[i, y_truth[i]] + 1e-100)
}
objective_values= c(-objective_value)
# learn W and v using gradient descent and online learning
iteration <- 1
while (1) {
print(paste0("running iteration#", iteration))
value=0
for (i in sample(N)) {
# calculate hidden nodes
Z[i,] <- sigmoid(c(1, X[i,]) %*% W)
# calculate output nodes
for (o in 1:num_classes){
y_predicted[i,o]= my_softmax(matrix(c(1, Z[i,]),1, H+1), v, o)
}
for (o in 1:num_classes){
if(y_truth[i] != o){
v[,o] <- v[,o] + eta * (0 - y_predicted[i, o]) * c(1, Z[i,])
} else {
v[,o] <- v[,o] + eta * (1 - y_predicted[i,o])* c(1, Z[i,])
}
}
for (h in 1:H){
#Intilizing cal value to do first summation
cal=0
for (t in 1:num_classes){
if(y_truth[i] == t){
cal= cal + (1 - y_predicted[i, t]) * v[h,t]
} else {
cal <- cal - y_predicted[i, t] * v[h,t]
}
}
W[,h] <- W[,h] + eta * cal * Z[i, h] * (1 - Z[i, h]) * c(1, X[i,])
}
#Calculating error function
for (s in 1:N){
class= y_truth[s]
value= value+ log(y_predicted[s,class] + 1e-100)
}
}
objective_values <- c(objective_values,- value)
if ( abs(objective_values[iteration + 1] - objective_values[iteration]) < epsilon || iteration >= max_iteration) {
break
}
iteration <- iteration + 1
}
plot(1:(iteration + 1),objective_values,
type = "l", lwd = 2, las = 1,
xlab = "Iteration", ylab = "Error")
plot(1:(iteration ),objective_values,
type = "l", lwd = 2, las = 1,
xlab = "Iteration", ylab = "Error")
### Machine Learning HW3 Developed by: Daliyah Aljamal,
##ID: 0063583
library(MASS)
set.seed(421)
# mean parameters
class_means <- array(c(2.0, 2.0,
-4.0,-4.0,
-2.0,2.0,
4.0, -4.0,
-2.0,-2.0,
4.0,4.0,
2.0,-2.0,
-4.0,4.0),c(2,1,8))
# Variance parameters
class_covariances <- array(c(+0.8, -0.6, -0.6,+0.8,
+0.4,+0.0,+0.0,+0.4,
+0.8,+ 0.6, +0.6,+0.8,
+0.4, +0.0,+0.0,+0.4,
+0.8, -0.6, -0.6, +0.8,
+0.4,+0.0,+0.0,+0.4,
+0.8,+0.6,+0.6,+0.8,
+0.4,+0.0,+0.0,+0.4), c(2,2,8))
# sample sizes
class_sizes <- c(100,100,100,100)
#Generate arandom Data
points1 <- mvrnorm(n = class_sizes[1]/2, mu = class_means[,,1], Sigma = class_covariances[,,1])
points2 <- mvrnorm(n = class_sizes[1]/2, mu = class_means[,,2], Sigma = class_covariances[,,2])
points3 <- mvrnorm(n = class_sizes[2]/2, mu = class_means[,,3], Sigma = class_covariances[,,3])
points4 <- mvrnorm(n = class_sizes[2]/2, mu = class_means[,,4], Sigma = class_covariances[,,4])
points5 <- mvrnorm(n = class_sizes[3]/2, mu = class_means[,,5], Sigma = class_covariances[,,5])
points6 <- mvrnorm(n = class_sizes[3]/2, mu = class_means[,,6], Sigma = class_covariances[,,6])
points7 <- mvrnorm(n = class_sizes[4]/2, mu = class_means[,,7], Sigma = class_covariances[,,7])
points8 <- mvrnorm(n = class_sizes[4]/2, mu = class_means[,,8], Sigma = class_covariances[,,8])
X <- rbind(points1, points2, points3, points4,points5,points6,points7,points8)
colnames(X) <- c("x1", "x2")
# generate corresponding labels
y <- c(rep(1, class_sizes[1]), rep(2, class_sizes[2]), rep(3, class_sizes[3]), rep(4, class_sizes[4]))
# write data to a file
write.csv(x = cbind(X, y), file = "ML_HW3.csv", row.names = FALSE)
# plot data points generated
plot(points1[,1], points1[,2], type = "p", pch = 19, col = "red", las = 1,
xlim = c(-6, 6), ylim = c(-6, 6),
xlab = "x1", ylab = "x2")
points(points2[,1], points2[,2], type = "p", pch = 19, col = "red")
points(points3[,1], points3[,2], type = "p", pch = 19, col = "green")
points(points4[,1], points4[,2], type = "p", pch = 19, col = "green")
points(points5[,1], points5[,2], type = "p", pch = 19, col = "blue")
points(points6[,1], points6[,2], type = "p", pch = 19, col = "blue")
points(points7[,1], points7[,2], type = "p", pch = 19, col = "pink")
points(points8[,1], points8[,2], type = "p", pch = 19, col = "pink")
# read data into memory
data_set <- read.csv("ML_HW3.csv")
# get X and y values
X <- cbind(data_set$x1, data_set$x2)
y_truth <- data_set$y
# get number of samples and number of features
N <- length(y_truth)
D <- ncol(X)
num_classes=max(y_truth)
#Define sigmoid Function
# define the sigmoid function
sigmoid <- function(a) {
return (1 / (1 + exp(-a)))
}
#Define Softmax Function
# #softmax output function
softmax<-function(X){
eps=1e-15
Eps=1-eps
M=max(X)
product=apply(X,2,function(x) exp(-M-log(rowSums(exp(X-M-x)))))
product=pmax(product,eps)
product=pmin(product,Eps)
return(product)
}
#MY SOFTMAX
my_softmax <- function(X,w, c)
{
sum <- 0
for (i in 1:num_classes)
{
sum <- sum+ exp(X %*% w[,i])
#next
}
return (exp(X %*% w[,c])/sum )
}
# set learning parameters
eta <- 0.1
epsilon <- 1e-3
H <- 20
max_iteration <- 200
# randomly initalize W and v
set.seed(421)
W <- matrix(runif((D + 1) * H, min = -0.01, max = 0.01), D + 1, H)
v <- matrix(runif((H + 1) * num_classes, min = -0.01, max = 0.01), H + 1, num_classes)
#Hidden Nodes
Z <- sigmoid(cbind(1,X) %*% W)
#Output Nodes
#y_predicted= my_softmax(cbind(1,Z),v,1)
y_predicted <- sapply(1:num_classes, function(i){my_softmax(cbind(rep(1, N), Z), v, i)})
objective_value =0
for (i in 1:N){
objective_value <- objective_value + log(y_predicted[i, y_truth[i]] + 1e-100)
}
objective_values= c(-objective_value)
# learn W and v using gradient descent and online learning
iteration <- 1
while (1) {
print(paste0("running iteration#", iteration))
value=0
for (i in sample(N)) {
# calculate hidden nodes
Z[i,] <- sigmoid(c(1, X[i,]) %*% W)
# calculate output nodes
for (o in 1:num_classes){
y_predicted[i,o]= my_softmax(matrix(c(1, Z[i,]),1, H+1), v, o)
}
for (o in 1:num_classes){
if(y_truth[i] != o){
v[,o] <- v[,o] + eta * (0 - y_predicted[i, o]) * c(1, Z[i,])
} else {
v[,o] <- v[,o] + eta * (1 - y_predicted[i,o])* c(1, Z[i,])
}
}
for (h in 1:H){
#Intilizing cal value to do first summation
cal=0
for (t in 1:num_classes){
if(y_truth[i] == t){
cal= cal + (1 - y_predicted[i, t]) * v[h,t]
} else {
cal <- cal - y_predicted[i, t] * v[h,t]
}
}
W[,h] <- W[,h] + eta * cal * Z[i, h] * (1 - Z[i, h]) * c(1, X[i,])
}
#Calculating error function
for (s in 1:N){
class= y_truth[s]
value= value+ log(y_predicted[s,class] + 1e-100)
}
}
objective_values <- c(objective_values,- value)
if ( abs(objective_values[iteration + 1] - objective_values[iteration]) < epsilon || iteration >= max_iteration) {
break
}
iteration <- iteration + 1
}
plot(1:(iteration +1),objective_values,
type = "l", lwd = 2, las = 1,
xlab = "Iteration", ylab = "Error")
y_pre_label<- c(rep(0),N,1)
for (t in 1:N){
y_pre_label[t]= which.max(y_predicted[t,])
# y_pre_label= rbind(y_pre_label,which.max(y_predicted[t,]))
}
confusion_matrix <- table(y_pre_label, y_truth)
print(confusion_matrix)
x1_interval <- seq(from = -6, to = +6, by = 0.06)
x2_interval <- seq(from = -6, to = +6, by = 0.06)
x1_grid <- matrix(x1_interval, nrow = length(x1_interval), ncol = length(x1_interval), byrow = FALSE)
x2_grid <- matrix(x2_interval, nrow = length(x2_interval), ncol = length(x2_interval), byrow = TRUE)
fun <- function(x1, x2, classes,n,z) {
Z <- sigmoid(cbind(1, x1, x2) %*% W)
my_y <- sapply(1:classes, function(c){
my_softmax(cbind(rep(1, N), Z), v, c)})
#returning class labels
my_y=which.max(my_y)
if(my_y == 4)
return(4)
if(my_y == 3)
return(3)
if(my_y == 2)
return(2)
else
return(1)
}
discriminant_values <- matrix(mapply(fun, x1_grid, x2_grid, num_classes, N, Z), nrow(x2_grid), ncol(x2_grid))
plot(X[y_truth == 1, 1], X[y_truth == 1, 2], type = "p", pch = 19, col = "red",
xlim = c(-6, +6),
ylim = c(-6, +6),
xlab = "x1", ylab = "x2", las = 1)
points(X[y_truth == 2, 1], X[y_truth == 2, 2], type = "p", pch = 19, col = "blue")
points(X[y_truth == 3, 1], X[y_truth == 3, 2], type = "p", pch = 19, col = "green")
points(X[y_truth == 4, 1], X[y_truth == 4, 2], type = "p", pch = 19, col = "magenta")
points(X[y_pre_label != y_truth, 1], X[y_pre_label != y_truth, 2], cex = 1.5, lwd = 2)
points(x1_grid[discriminant_values == 1], x2_grid[discriminant_values == 1], col = rgb(red = 1, green = 0, blue = 0, alpha = 0.01), pch = 16)
points(x1_grid[discriminant_values == 2], x2_grid[discriminant_values == 2], col = rgb(red = 0, green = 0, blue = 1, alpha = 0.01), pch = 16)
points(x1_grid[discriminant_values == 3], x2_grid[discriminant_values == 3], col = rgb(red = 0, green = 1, blue = 0, alpha = 0.01), pch = 16)
points(x1_grid[discriminant_values == 4], x2_grid[discriminant_values == 4], col = rgb(red = 0.5, green = 0, blue = 0.5, alpha = 0.01), pch = 16)
contour(x1_interval, x2_interval, discriminant_values, levels = c(3), add = TRUE, lwd = 2, drawlabels = FALSE)
contour(x1_interval, x2_interval, discriminant_values, levels = c(2), add = TRUE, lwd = 2, drawlabels = FALSE)
contour(x1_interval, x2_interval, discriminant_values, levels = c(4), add = TRUE, lwd = 2, drawlabels = FALSE)
contour(x1_interval, x2_interval, discriminant_values, levels = c(1), add = TRUE, lwd = 2, drawlabels = FALSE)
